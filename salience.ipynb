{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cc4ff8",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [7]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dbb8fa2-4b9c-4e74-9bc8-fb1006f5d661",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-21T00:35:07.120874Z",
     "iopub.status.busy": "2023-03-21T00:35:07.119278Z",
     "iopub.status.idle": "2023-03-21T00:35:07.134067Z",
     "shell.execute_reply": "2023-03-21T00:35:07.132998Z"
    },
    "papermill": {
     "duration": 0.032774,
     "end_time": "2023-03-21T00:35:07.135844",
     "exception": false,
     "start_time": "2023-03-21T00:35:07.103070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#| hide\n",
    "import sys\n",
    "sys.path.append('../.venv/lib/python3.9/site-packages/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac55b2fd-5409-4aae-b3e3-aaecf4fa8190",
   "metadata": {
    "papermill": {
     "duration": 0.023461,
     "end_time": "2023-03-21T00:35:07.166544",
     "exception": false,
     "start_time": "2023-03-21T00:35:07.143083",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Salience Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1cdd26-424c-4d87-b5c5-c51458b64950",
   "metadata": {
    "papermill": {
     "duration": 0.007984,
     "end_time": "2023-03-21T00:35:07.182966",
     "exception": false,
     "start_time": "2023-03-21T00:35:07.174982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model salience has been used extensively in the XAI community to help interpret why models make their decisions. One of the most popular salience methods is Gradient-Weighted Class Activation Mappings (Grad-CAMs), which visulaizes the gradients of the predicted class at the final convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78ec608-8034-4d9f-99b2-b948c5907e95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-21T00:35:07.206289Z",
     "iopub.status.busy": "2023-03-21T00:35:07.205275Z",
     "iopub.status.idle": "2023-03-21T00:35:09.918714Z",
     "shell.execute_reply": "2023-03-21T00:35:09.917759Z"
    },
    "papermill": {
     "duration": 2.727749,
     "end_time": "2023-03-21T00:35:09.922871",
     "exception": false,
     "start_time": "2023-03-21T00:35:07.195122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/afs/crc.nd.edu/group/TAI/Users/painswor/Explain2Me-Framework-Example/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import dvc.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2936f132-99e8-4c0d-b23b-ba627a8103ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-21T00:35:09.957539Z",
     "iopub.status.busy": "2023-03-21T00:35:09.957030Z",
     "iopub.status.idle": "2023-03-21T00:35:09.964630Z",
     "shell.execute_reply": "2023-03-21T00:35:09.962987Z"
    },
    "papermill": {
     "duration": 0.029821,
     "end_time": "2023-03-21T00:35:09.967330",
     "exception": false,
     "start_time": "2023-03-21T00:35:09.937509",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "FILENAME: str = \"ffhq_aligned/00001.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18ba7d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-21T00:35:10.019702Z",
     "iopub.status.busy": "2023-03-21T00:35:10.019168Z",
     "iopub.status.idle": "2023-03-21T00:35:10.026712Z",
     "shell.execute_reply": "2023-03-21T00:35:10.024755Z"
    },
    "papermill": {
     "duration": 0.037515,
     "end_time": "2023-03-21T00:35:10.030637",
     "exception": false,
     "start_time": "2023-03-21T00:35:09.993122",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "FILENAME = \"StyleGAN-1/000700.png\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304d665-ec79-4833-9f8b-869f7986a661",
   "metadata": {
    "papermill": {
     "duration": 0.00782,
     "end_time": "2023-03-21T00:35:10.049404",
     "exception": false,
     "start_time": "2023-03-21T00:35:10.041584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Necessary Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63ef34ec-954c-4bab-91f1-75407e744141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-21T00:35:10.065771Z",
     "iopub.status.busy": "2023-03-21T00:35:10.064562Z",
     "iopub.status.idle": "2023-03-21T00:35:10.072668Z",
     "shell.execute_reply": "2023-03-21T00:35:10.070990Z"
    },
    "papermill": {
     "duration": 0.020351,
     "end_time": "2023-03-21T00:35:10.076600",
     "exception": false,
     "start_time": "2023-03-21T00:35:10.056249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = \"../models/weights/Densenet/densenet_cyborg_1/Logs/final_model.pth\"\n",
    "device = torch.device('cpu')\n",
    "network = \"densenet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78eae4d-6a20-43a2-9071-86d10d498167",
   "metadata": {
    "papermill": {
     "duration": 0.007628,
     "end_time": "2023-03-21T00:35:10.095442",
     "exception": false,
     "start_time": "2023-03-21T00:35:10.087814",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4905be50-dd74-426d-9229-14011d399567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-21T00:35:10.113055Z",
     "iopub.status.busy": "2023-03-21T00:35:10.111775Z",
     "iopub.status.idle": "2023-03-21T00:35:10.653717Z",
     "shell.execute_reply": "2023-03-21T00:35:10.652360Z"
    },
    "papermill": {
     "duration": 0.555964,
     "end_time": "2023-03-21T00:35:10.657415",
     "exception": false,
     "start_time": "2023-03-21T00:35:10.101451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a4d7a3-1eb2-4344-9550-bbbfc25761de",
   "metadata": {
    "papermill": {
     "duration": 0.007049,
     "end_time": "2023-03-21T00:35:10.677635",
     "exception": false,
     "start_time": "2023-03-21T00:35:10.670586",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "First, load in the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa66bcf3",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f266322-e036-46e1-a014-d011d6bddac7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-21T00:35:10.694896Z",
     "iopub.status.busy": "2023-03-21T00:35:10.693640Z",
     "iopub.status.idle": "2023-03-21T00:35:11.290393Z",
     "shell.execute_reply": "2023-03-21T00:35:11.289388Z"
    },
    "papermill": {
     "duration": 0.60876,
     "end_time": "2023-03-21T00:35:11.294060",
     "exception": true,
     "start_time": "2023-03-21T00:35:10.685300",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../models/weights/Densenet/densenet_cyborg_1/Logs/final_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/afs/crc.nd.edu/group/TAI/Users/painswor/Explain2Me-Framework-Example/.venv/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m/afs/crc.nd.edu/group/TAI/Users/painswor/Explain2Me-Framework-Example/.venv/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m/afs/crc.nd.edu/group/TAI/Users/painswor/Explain2Me-Framework-Example/.venv/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../models/weights/Densenet/densenet_cyborg_1/Logs/final_model.pth'"
     ]
    }
   ],
   "source": [
    "weights = torch.load(model_path, map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b975c785-ebfa-46b9-8bef-d982a89a4ec2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now, load weights on a single binary DesNet121 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68966569-09f1-4f7a-a2d9-a68221fe0f91",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if network == \"resnet\":\n",
    "    im_size = 224\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "elif network == \"inception\":\n",
    "    im_size = 299\n",
    "    model = models.inception_v3(pretrained=True,aux_logits=False)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 2)\n",
    "elif network == \"xception\":\n",
    "    im_size = 299\n",
    "    model, *_ = model_selection(modelname='xception', num_out_classes=2)\n",
    "else: # else DenseNet\n",
    "    im_size = 224\n",
    "    model = models.densenet121(pretrained=True)\n",
    "    num_ftrs = model.classifier.in_features\n",
    "    model.classifier = nn.Linear(num_ftrs, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbf922b-4bfe-4b42-9b41-c00e7eff8338",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_layers = [model.features[-1]]\n",
    "model.load_state_dict(weights['state_dict'])\n",
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f748c0-6dd4-4151-9ded-42ae008c9bd9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5071c411-70a1-4f2a-9d21-793a003b2d9a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a419fc8a-4c15-400d-8928-924edb2cb84e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if network == \"xception\":\n",
    "    # Transformation specified for the pre-processing\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize([im_size, im_size]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "            ])\n",
    "else:\n",
    "    # Transformation specified for the pre-processing\n",
    "    transform = transforms.Compose([\n",
    "                transforms.Resize([im_size, im_size]),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64752a73-474b-4d91-aa0f-697899f061bc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "imageScores = []\n",
    "sigmoid = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacdd9b5-cac4-4eb9-81d2-8cc902cb227f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Creating GradCam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cacac12-0aa8-46f9-a3be-11f262c83ac9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchcam.methods import GradCAM\n",
    "from torchcam.methods import GradCAMpp\n",
    "import skimage.io"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978cb681-834a-40a1-b5ff-3c13b8b84010",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Laod the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dcaa1d-6d6e-4619-9cd7-7b6b331cd033",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gradcamType = \"gradcam\"\n",
    "img_path = f'../data/images/{FILENAME}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b13172-5d17-4cdc-9bf3-df0b4187de05",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading in the image\n",
    "image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "# Image transformation\n",
    "input_tensor = transform(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e069c7e-a2e1-460d-8494-f921488d5180",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Generate GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead47be-7f28-41fb-801b-e9b01a015070",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if gradcamType == \"gradcam\":\n",
    "    #cam_extractor = GradCAM(model, target_layers)\n",
    "    cam_extractor = GradCAM(model)\n",
    "elif gradcamType == \"gradcamPP\":\n",
    "    cam_extractor = GradCAMpp(model, target_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c0c65-6008-43cc-97d4-d94a6aeaeeab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Processing GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6828e1b7-8596-420a-b877-d9d8fea5a8dd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import normalize, resize, to_pil_image\n",
    "from torchcam.utils import overlay_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6f40c6-b4c2-4530-90e1-0b58960c3c28",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Preprocess your data and feed it to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e88a74-092f-453b-a3d2-107f6d16cbc7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "size = 'upscaled'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc84a5-d299-4a4f-9788-0080729a1b16",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = model(input_tensor.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94edf158-2aae-4101-9761-044babcd61be",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Retrieve the CAM by passing class index and model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc46e1f-07ca-49e9-98cd-b37e93b1d97a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "activation_map = cam_extractor(out.squeeze(0).argmax().item(), out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935a00a-16be-4882-a027-3109ca4d81f3",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cam = to_pil_image(activation_map[0].squeeze(0), mode='F')\n",
    "cam = overlay_mask(image, cam, alpha=0.50)\n",
    "if size == 'upscaled':\n",
    "    cam = cam.resize((im_size, im_size), resample = Image.BICUBIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72562571-bf3f-4d72-841f-e6dd08e75487",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "## Display GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76431a-1d87-456b-af4f-367c6f71ea07",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc4bdd-db50-4f84-9ed7-07f1c7db9495",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_image = Image.open(img_path).convert('RGB')\n",
    "overlayed_cam = cam\n",
    "\n",
    "fig = plt.figure(figsize=(10., 8.), facecolor='white')\n",
    "grid = ImageGrid(fig, 111,  # similar to subplot(111)\n",
    "                 nrows_ncols=(1, 2),  # creates 2x2 grid of axes\n",
    "                 axes_pad=0.1,  # pad between axes in inch.\n",
    "                 aspect=1.2\n",
    "                 )\n",
    "\n",
    "for index, (ax, im) in enumerate(zip(grid, [original_image, overlayed_cam])):\n",
    "    # Iterating over the grid returns the Axes.\n",
    "    ax.axis('off')\n",
    "    ax.grid(None)\n",
    "    ax.imshow(im)\n",
    "    #ax.set_title(predictions[index], fontsize=45)\n",
    "fig.set_size_inches(32, 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe49aea-8fff-46f7-85b3-c12f66ccef12",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_directory = f'../output/cams/{img_name}'\n",
    "cam.save(save_directory)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.386839,
   "end_time": "2023-03-21T00:35:14.282535",
   "environment_variables": {},
   "exception": true,
   "input_path": "nbs/salience.ipynb",
   "output_path": "salience.ipynb",
   "parameters": {
    "FILENAME": "StyleGAN-1/000700.png"
   },
   "start_time": "2023-03-21T00:35:04.895696",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
